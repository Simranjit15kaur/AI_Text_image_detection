{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfd3oL8iVENM",
        "outputId": "9f219f40-7158-4d2b-a345-2d7f348ca4b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.7-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cmudict (from textstat)\n",
            "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.7.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.22.0)\n",
            "Downloading textstat-0.7.7-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, cmudict, textstat\n",
            "Successfully installed cmudict-1.0.32 pyphen-0.17.2 textstat-0.7.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required Python libraries using pip.\n",
        "# - gradio: to create web-based user interfaces for machine learning apps\n",
        "# - textstat: to calculate readability scores and text statistics\n",
        "# - exifread: to extract metadata (EXIF data) from images\n",
        "# - plotly: to create interactive visualizations\n",
        "!pip install gradio textstat exifread plotly\n",
        "\n",
        "# Import the Natural Language Toolkit (NLTK), a library for text processing\n",
        "import nltk\n",
        "\n",
        "# Download the 'punkt' tokenizer model for sentence and word tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download the list of common stopwords (e.g., \"the\", \"is\", \"and\") in English\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download the WordNet lexical database for lemmatization and synonym lookup\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRNbOju0WlR3",
        "outputId": "f4ff8516-559a-4382-9589-22fff6e9e71b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.11/dist-packages (0.7.7)\n",
            "Collecting exifread\n",
            "  Downloading exifread-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.32.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.11/dist-packages (from textstat) (0.17.2)\n",
            "Requirement already satisfied: cmudict in /usr/local/lib/python3.11/dist-packages (from textstat) (1.0.32)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.7.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading exifread-3.3.1-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: exifread\n",
            "Successfully installed exifread-3.3.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "2tXmCxxaGLNP",
        "outputId": "f1560411-c834-4c35-8ca5-15e277e9ccbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 306MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3c183f173347c49e74.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3c183f173347c49e74.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Standard and OS-related\n",
        "import os  # Provides a way of using operating system dependent functionality like reading file paths\n",
        "import logging  # Used to log messages for debugging or tracking\n",
        "import math  # Provides mathematical functions like floor, ceil, etc.\n",
        "\n",
        "# Computer Vision and Image Handling\n",
        "import cv2  # OpenCV library for image and video processing\n",
        "import numpy as np  # Fundamental package for numerical computing and arrays\n",
        "from PIL import Image  # Python Imaging Library for image manipulation\n",
        "\n",
        "# PyTorch and torchvision for deep learning and image models\n",
        "import torch  # Core PyTorch library\n",
        "import torchvision.models as models  # Pretrained models like ResNet, VGG, etc.\n",
        "import torchvision.transforms as transforms  # Common image transformations (resize, normalize, etc.)\n",
        "\n",
        "# Scipy for scientific computing and statistics\n",
        "from scipy import stats  # For statistical functions like mode, z-score, etc.\n",
        "\n",
        "# JSON handling\n",
        "import json  # Read and write JSON data\n",
        "\n",
        "# Typing for better type hinting in functions\n",
        "from typing import Dict, List, Tuple, Union, Any\n",
        "\n",
        "# Suppress warnings if needed\n",
        "import warnings  # Issue warning messages (can be used to suppress or customize warnings)\n",
        "\n",
        "# Text and String processing\n",
        "import re  # Regular expressions for pattern matching in strings\n",
        "import unicodedata  # Handle Unicode characters, useful in text normalization\n",
        "import string  # String utilities like punctuation, ascii_letters, etc.\n",
        "\n",
        "# NLTK – Natural Language Toolkit\n",
        "import nltk  # Base NLTK package for text processing\n",
        "from nltk.corpus import stopwords, wordnet  # Stopwords for filtering; WordNet for lexical database\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize  # Sentence and word tokenizers\n",
        "\n",
        "# Counter from collections for word frequency counting\n",
        "from collections import Counter  # Count occurrences of elements in an iterable (like words)\n",
        "\n",
        "# Plotly for interactive plotting\n",
        "import plotly.graph_objects as go  # For low-level chart building\n",
        "import plotly.express as px  # High-level interface for quick plotting\n",
        "\n",
        "# Textstat for readability scores\n",
        "from textstat import flesch_reading_ease  # Readability score metric (Flesch Reading Ease)\n",
        "\n",
        "\n",
        "# Gradio specific import\n",
        "import gradio as gr  # Gradio is a Python library used to build simple and interactive web interfaces\n",
        "                     # for machine learning models, data apps, or any Python function.\n",
        "\n",
        "# Suppress all warnings (optional)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # This line disables all warning messages that Python or libraries may show.\n",
        "                                   # It helps make the output cleaner, but use it cautiously during debugging.\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Custom JSON encoder class for handling NumPy-specific data types\n",
        "# JSON (JavaScript Object Notation) is a common format for saving or transmitting data.\n",
        "# However, NumPy data types (like np.int32, np.float64, np.ndarray) are not JSON serializable by default.\n",
        "# This custom encoder converts them into native Python types so they can be properly saved or shared.\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):  # Inheriting from the built-in JSONEncoder class\n",
        "    def default(self, obj):\n",
        "        # If the object is a NumPy integer (e.g., np.int32, np.int64), convert it to a regular Python int\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "\n",
        "        # If the object is a NumPy float (e.g., np.float32, np.float64), convert it to a regular Python float\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "\n",
        "        # If the object is a NumPy array (e.g., np.array([...]), convert it to a regular Python list\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "\n",
        "        # If the object is a NumPy boolean (e.g., np.bool_), convert it to a regular Python bool\n",
        "        elif isinstance(obj, bool):\n",
        "            return bool(obj)\n",
        "\n",
        "        # For all other data types, use the default encoding behavior of the parent class\n",
        "        return super(NumpyEncoder, self).default(obj)\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ==================== IMAGE DETECTION ====================\n",
        "\n",
        "class ImageDetector:\n",
        "    def __init__(self):\n",
        "        # Initialize the model for feature extraction\n",
        "        try:\n",
        "            self.model = models.resnet50(pretrained=True)\n",
        "            self.model = torch.nn.Sequential(*list(self.model.children())[:-1])  # Remove last layer\n",
        "            self.model.eval()\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load ResNet model: {e}. Using fallback methods.\")\n",
        "            self.model = None\n",
        "\n",
        "        # Define image transformations\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Initialize feature weights\n",
        "        self.feature_weights = {\n",
        "            'noise_analysis': 0.3,\n",
        "            'metadata_analysis': 0.2,\n",
        "            'deep_features': 0.3,\n",
        "            'statistical_analysis': 0.2\n",
        "        }\n",
        "\n",
        "    def analyze_image(self, image_path: str) -> Dict:\n",
        "        \"\"\"Analyze an image to determine if it's AI-generated.\"\"\"\n",
        "        try:\n",
        "            # Load and preprocess image\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            img_array = np.array(image)\n",
        "\n",
        "            # Perform various analyses\n",
        "            noise_score = self._analyze_noise_patterns(img_array)\n",
        "            metadata_score = self._analyze_metadata(image_path)\n",
        "            deep_features = self._extract_deep_features(image)\n",
        "            statistical_score = self._analyze_statistical_features(img_array)\n",
        "\n",
        "            # Calculate AI indicators\n",
        "            ai_indicators = {\n",
        "                'high_noise_consistency': bool(noise_score > 0.7),\n",
        "                'missing_metadata': bool(metadata_score > 0.6),\n",
        "                'deep_feature_match': bool(deep_features > 0.65),\n",
        "                'statistical_anomaly': bool(statistical_score > 0.6)\n",
        "            }\n",
        "\n",
        "            # Calculate final score\n",
        "            final_score = float(\n",
        "                self.feature_weights['noise_analysis'] * noise_score +\n",
        "                self.feature_weights['metadata_analysis'] * metadata_score +\n",
        "                self.feature_weights['deep_features'] * deep_features +\n",
        "                self.feature_weights['statistical_analysis'] * statistical_score\n",
        "            )\n",
        "\n",
        "            # Add AI indicator bonus\n",
        "            ai_indicator_bonus = sum(ai_indicators.values()) * 0.1\n",
        "            final_score = min(final_score + ai_indicator_bonus, 1.0)\n",
        "\n",
        "            return {\n",
        "                'is_ai_generated': bool(final_score > 0.55),\n",
        "                'confidence_score': float(final_score),\n",
        "                'ai_indicators': ai_indicators,\n",
        "                'detailed_scores': {\n",
        "                    'noise_analysis': float(noise_score),\n",
        "                    'metadata_analysis': float(metadata_score),\n",
        "                    'deep_features': float(deep_features),\n",
        "                    'statistical_analysis': float(statistical_score)\n",
        "                },\n",
        "                'visualizations': self._generate_image_visualizations(\n",
        "                    noise_score, metadata_score, deep_features, statistical_score, final_score\n",
        "                )\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing image: {str(e)}\")\n",
        "            return {\n",
        "                'is_ai_generated': False,\n",
        "                'confidence_score': 0.0,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def _analyze_noise_patterns(self, img_array: np.ndarray) -> float:\n",
        "        \"\"\"Analyze noise patterns in the image.\"\"\"\n",
        "        try:\n",
        "            # Convert to grayscale for analysis\n",
        "            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            # Calculate noise metrics\n",
        "            # 1. High frequency noise analysis\n",
        "            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "\n",
        "            # 2. Texture analysis using local binary patterns\n",
        "            def local_binary_pattern(image, radius=1, n_points=8):\n",
        "                h, w = image.shape\n",
        "                lbp = np.zeros((h, w), dtype=np.uint8)\n",
        "                for i in range(radius, h - radius):\n",
        "                    for j in range(radius, w - radius):\n",
        "                        center = image[i, j]\n",
        "                        code = 0\n",
        "                        for k in range(n_points):\n",
        "                            angle = 2 * np.pi * k / n_points\n",
        "                            x = int(i + radius * np.cos(angle))\n",
        "                            y = int(j + radius * np.sin(angle))\n",
        "                            if 0 <= x < h and 0 <= y < w:\n",
        "                                if image[x, y] >= center:\n",
        "                                    code |= (1 << k)\n",
        "                        lbp[i, j] = code\n",
        "                return lbp\n",
        "\n",
        "            lbp = local_binary_pattern(gray)\n",
        "            lbp_hist = np.histogram(lbp.ravel(), bins=256)[0]\n",
        "            lbp_uniformity = np.sum(lbp_hist ** 2) / (lbp_hist.sum() ** 2)\n",
        "\n",
        "            # 3. Frequency domain analysis\n",
        "            f_transform = np.fft.fft2(gray)\n",
        "            f_shift = np.fft.fftshift(f_transform)\n",
        "            magnitude_spectrum = np.abs(f_shift)\n",
        "\n",
        "            # Calculate energy distribution\n",
        "            center = np.array(magnitude_spectrum.shape) // 2\n",
        "            y, x = np.ogrid[:magnitude_spectrum.shape[0], :magnitude_spectrum.shape[1]]\n",
        "            mask = (x - center[1])**2 + (y - center[0])**2 <= (min(center) * 0.3)**2\n",
        "\n",
        "            low_freq_energy = np.sum(magnitude_spectrum[mask])\n",
        "            total_energy = np.sum(magnitude_spectrum)\n",
        "            freq_ratio = low_freq_energy / total_energy if total_energy > 0 else 0\n",
        "\n",
        "            # Combine metrics\n",
        "            noise_score = 0.0\n",
        "\n",
        "            # High Laplacian variance suggests less AI processing\n",
        "            if laplacian_var > 1000:\n",
        "                noise_score += 0.3\n",
        "            elif laplacian_var < 100:\n",
        "                noise_score += 0.7  # Very smooth, likely AI\n",
        "\n",
        "            # High LBP uniformity suggests AI generation\n",
        "            if lbp_uniformity > 0.8:\n",
        "                noise_score += 0.4\n",
        "\n",
        "            # Frequency analysis\n",
        "            if freq_ratio > 0.8:  # Too much low frequency content\n",
        "                noise_score += 0.3\n",
        "\n",
        "            return min(noise_score, 1.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error in noise analysis: {e}\")\n",
        "            return 0.5  # Default score if analysis fails\n",
        "\n",
        "    def _analyze_metadata(self, image_path: str) -> float:\n",
        "        \"\"\"Analyze image metadata for AI generation indicators.\"\"\"\n",
        "        try:\n",
        "            # Try to read EXIF data\n",
        "            try:\n",
        "                import exifread\n",
        "                with open(image_path, 'rb') as f:\n",
        "                    tags = exifread.process_file(f)\n",
        "            except ImportError:\n",
        "                logger.warning(\"exifread not available, using PIL for metadata\")\n",
        "                image = Image.open(image_path)\n",
        "                tags = image._getexif() or {}\n",
        "\n",
        "            metadata_score = 0.0\n",
        "\n",
        "            # Check for missing common metadata\n",
        "            common_tags = ['DateTime', 'Camera', 'Software', 'GPS']\n",
        "            missing_tags = 0\n",
        "\n",
        "            if isinstance(tags, dict):\n",
        "                tag_keys = [str(key).lower() for key in tags.keys()]\n",
        "                for tag in common_tags:\n",
        "                    if not any(tag.lower() in key for key in tag_keys):\n",
        "                        missing_tags += 1\n",
        "            else:\n",
        "                missing_tags = len(common_tags)  # Assume all missing if no tags\n",
        "\n",
        "            # More missing metadata = higher AI score\n",
        "            metadata_score += (missing_tags / len(common_tags)) * 0.6\n",
        "\n",
        "            # Check for AI-specific software signatures\n",
        "            software_indicators = ['adobe', 'midjourney', 'dall-e', 'stable diffusion', 'gpt']\n",
        "            if isinstance(tags, dict):\n",
        "                for key, value in tags.items():\n",
        "                    value_str = str(value).lower()\n",
        "                    for indicator in software_indicators:\n",
        "                        if indicator in value_str:\n",
        "                            metadata_score += 0.4\n",
        "                            break\n",
        "\n",
        "            # Check file properties\n",
        "            image = Image.open(image_path)\n",
        "\n",
        "            # Common AI generation resolutions\n",
        "            ai_resolutions = [(512, 512), (1024, 1024), (768, 768), (1024, 768)]\n",
        "            if image.size in ai_resolutions:\n",
        "                metadata_score += 0.2\n",
        "\n",
        "            # Check for perfect aspect ratios (common in AI)\n",
        "            width, height = image.size\n",
        "            aspect_ratio = width / height\n",
        "            if abs(aspect_ratio - 1.0) < 0.01 or abs(aspect_ratio - 1.5) < 0.01:\n",
        "                metadata_score += 0.1\n",
        "\n",
        "            return min(metadata_score, 1.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error in metadata analysis: {e}\")\n",
        "            return 0.5\n",
        "\n",
        "    def _extract_deep_features(self, image: Image.Image) -> float:\n",
        "        \"\"\"Extract deep features using ResNet model.\"\"\"\n",
        "        try:\n",
        "            if self.model is None:\n",
        "                return 0.5  # Default if model not available\n",
        "\n",
        "            # Preprocess image\n",
        "            input_tensor = self.transform(image).unsqueeze(0)\n",
        "\n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                features = self.model(input_tensor)\n",
        "                features = features.squeeze().numpy()\n",
        "\n",
        "            # Analyze feature characteristics\n",
        "            feature_score = 0.0\n",
        "\n",
        "            # Calculate feature statistics\n",
        "            mean_activation = np.mean(features)\n",
        "            std_activation = np.std(features)\n",
        "            sparsity = np.sum(features == 0) / len(features)\n",
        "\n",
        "            # AI-generated images often have different activation patterns\n",
        "            # These thresholds are heuristic and would need validation\n",
        "            if mean_activation > 0.5:  # High average activation\n",
        "                feature_score += 0.3\n",
        "\n",
        "            if std_activation < 0.2:  # Low variation in activations\n",
        "                feature_score += 0.3\n",
        "\n",
        "            if sparsity > 0.7:  # High sparsity\n",
        "                feature_score += 0.2\n",
        "\n",
        "            # Analyze feature distribution\n",
        "            hist, _ = np.histogram(features, bins=50)\n",
        "            hist_normalized = hist / np.sum(hist)\n",
        "            entropy = -np.sum(hist_normalized * np.log(hist_normalized + 1e-10))\n",
        "\n",
        "            # Lower entropy might indicate AI generation\n",
        "            if entropy < 3.0:\n",
        "                feature_score += 0.2\n",
        "\n",
        "            return min(feature_score, 1.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error in deep feature extraction: {e}\")\n",
        "            return 0.5\n",
        "\n",
        "    def _analyze_statistical_features(self, img_array: np.ndarray) -> float:\n",
        "        \"\"\"Analyze statistical features of the image.\"\"\"\n",
        "        try:\n",
        "            statistical_score = 0.0\n",
        "\n",
        "            # Color distribution analysis\n",
        "            for channel in range(3):  # RGB channels\n",
        "                channel_data = img_array[:, :, channel].flatten()\n",
        "\n",
        "                # Calculate statistics\n",
        "                mean_val = np.mean(channel_data)\n",
        "                std_val = np.std(channel_data)\n",
        "                skewness = stats.skew(channel_data)\n",
        "                kurtosis = stats.kurtosis(channel_data)\n",
        "\n",
        "                # AI images often have specific statistical properties\n",
        "                # Normal distribution characteristics\n",
        "                if abs(skewness) < 0.1:  # Very symmetric\n",
        "                    statistical_score += 0.1\n",
        "\n",
        "                if abs(kurtosis) < 0.1:  # Normal kurtosis\n",
        "                    statistical_score += 0.1\n",
        "\n",
        "                # Unusual standard deviation\n",
        "                if std_val < 20 or std_val > 80:\n",
        "                    statistical_score += 0.1\n",
        "\n",
        "            # Edge analysis\n",
        "            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
        "            edges = cv2.Canny(gray, 100, 200)\n",
        "            edge_density = np.sum(edges > 0) / edges.size\n",
        "\n",
        "            # AI images often have specific edge characteristics\n",
        "            if edge_density < 0.05 or edge_density > 0.3:\n",
        "                statistical_score += 0.2\n",
        "\n",
        "            # Gradient analysis\n",
        "            grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "            grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "            gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
        "\n",
        "            grad_mean = np.mean(gradient_magnitude)\n",
        "            grad_std = np.std(gradient_magnitude)\n",
        "\n",
        "            # Unusual gradient patterns\n",
        "            if grad_std / (grad_mean + 1e-10) > 2.0:\n",
        "                statistical_score += 0.2\n",
        "\n",
        "            return min(statistical_score, 1.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error in statistical analysis: {e}\")\n",
        "            return 0.5\n",
        "\n",
        "    def _generate_image_visualizations(self, noise_score, metadata_score, deep_features, statistical_score, final_score):\n",
        "        \"\"\"Generate visualizations for image analysis results.\"\"\"\n",
        "        visualizations = {}\n",
        "\n",
        "        try:\n",
        "            # Scores bar chart\n",
        "            scores = {\n",
        "                'Noise Analysis': noise_score,\n",
        "                'Metadata Analysis': metadata_score,\n",
        "                'Deep Features': deep_features,\n",
        "                'Statistical Analysis': statistical_score\n",
        "            }\n",
        "\n",
        "            fig_scores = px.bar(\n",
        "                x=list(scores.keys()),\n",
        "                y=list(scores.values()),\n",
        "                labels={'x': 'Analysis Type', 'y': 'Score'},\n",
        "                title='Image Analysis Scores',\n",
        "                color=list(scores.values()),\n",
        "                color_continuous_scale='RdYlBu_r'\n",
        "            )\n",
        "            fig_scores.update_layout(showlegend=False)\n",
        "            visualizations['analysis_scores'] = fig_scores\n",
        "\n",
        "            # Confidence gauge\n",
        "            fig_gauge = go.Figure(go.Indicator(\n",
        "                mode=\"gauge+number\",\n",
        "                value=final_score,\n",
        "                domain={'x': [0, 1], 'y': [0, 1]},\n",
        "                title={'text': \"AI Generation Confidence\"},\n",
        "                gauge={'axis': {'range': [0, 1]},\n",
        "                       'steps': [\n",
        "                           {'range': [0, 0.3], 'color': \"lightgreen\"},\n",
        "                           {'range': [0.3, 0.7], 'color': \"yellow\"},\n",
        "                           {'range': [0.7, 1], 'color': \"lightcoral\"}],\n",
        "                       'threshold': {'line': {'color': \"red\", 'width': 4}, 'thickness': 0.75, 'value': 0.55}}\n",
        "            ))\n",
        "            visualizations['confidence_gauge'] = fig_gauge\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error generating visualizations: {e}\")\n",
        "\n",
        "        return visualizations\n",
        "\n",
        "# ==================== TEXT DETECTION ====================\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.contractions = {\n",
        "            \"n't\": \" not\", \"'ll\": \" will\", \"'re\": \" are\", \"'ve\": \" have\",\n",
        "            \"'m\": \" am\", \"'d\": \" would\", \"'s\": \" is\", \"won't\": \"will not\",\n",
        "            \"can't\": \"cannot\", \"shouldn't\": \"should not\", \"wouldn't\": \"would not\"\n",
        "        }\n",
        "\n",
        "    def preprocess(self, text: str) -> str:\n",
        "        \"\"\"Preprocess text for analysis.\"\"\"\n",
        "        text = str(text)\n",
        "        text = self._normalize_unicode(text)\n",
        "        text = self._remove_urls(text)\n",
        "        text = self._remove_emails(text)\n",
        "        text = self._remove_html_tags(text)\n",
        "        text = self._expand_contractions(text)\n",
        "        text = self._normalize_whitespace(text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _normalize_unicode(self, text: str) -> str:\n",
        "        \"\"\"Normalize unicode characters.\"\"\"\n",
        "        return unicodedata.normalize('NFKD', text)\n",
        "\n",
        "    def _remove_urls(self, text: str) -> str:\n",
        "        \"\"\"Remove URLs from text.\"\"\"\n",
        "        url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "        return url_pattern.sub('', text)\n",
        "\n",
        "    def _remove_emails(self, text: str) -> str:\n",
        "        \"\"\"Remove email addresses from text.\"\"\"\n",
        "        email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
        "        return email_pattern.sub('', text)\n",
        "\n",
        "    def _remove_html_tags(self, text: str) -> str:\n",
        "        \"\"\"Remove HTML tags from text.\"\"\"\n",
        "        html_pattern = re.compile(r'<[^>]+>')\n",
        "        return html_pattern.sub('', text)\n",
        "\n",
        "    def _expand_contractions(self, text: str) -> str:\n",
        "        \"\"\"Expand contractions in text.\"\"\"\n",
        "        for contraction, expansion in self.contractions.items():\n",
        "            text = text.replace(contraction, expansion)\n",
        "        return text\n",
        "\n",
        "    def _normalize_whitespace(self, text: str) -> str:\n",
        "        \"\"\"Normalize whitespace in text.\"\"\"\n",
        "        return re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "class WatermarkDetector:\n",
        "    def __init__(self):\n",
        "        self.patterns = {\n",
        "            'repetition': self._check_repetition,\n",
        "            'perplexity': self._check_perplexity,\n",
        "            'token_patterns': self._check_token_patterns,\n",
        "            'sentence_structure': self._check_sentence_structure\n",
        "        }\n",
        "        self.ai_markers = [\n",
        "            \"as an ai language model\", \"i am an ai\", \"i cannot\", \"i don't have\",\n",
        "            \"i don't know\", \"i'm not sure\", \"i'm unable to\", \"i apologize\",\n",
        "            \"i'm sorry\", \"as an artificial intelligence\", \"i'm an ai assistant\"\n",
        "        ]\n",
        "\n",
        "    def detect(self, text: str) -> float:\n",
        "        \"\"\"Detect watermarks in text.\"\"\"\n",
        "        try:\n",
        "            watermark_score = 0.0\n",
        "            text_lower = text.lower()\n",
        "\n",
        "            # Check for AI markers\n",
        "            marker_count = 0\n",
        "            for marker in self.ai_markers:\n",
        "                if marker in text_lower:\n",
        "                    marker_count += 1\n",
        "\n",
        "            if marker_count > 0:\n",
        "                watermark_score += min(marker_count * 0.2, 0.8)\n",
        "\n",
        "            # Check patterns\n",
        "            for pattern_name, pattern_func in self.patterns.items():\n",
        "                try:\n",
        "                    pattern_score = pattern_func(text)\n",
        "                    watermark_score += pattern_score * 0.1\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error in pattern {pattern_name}: {e}\")\n",
        "\n",
        "            return min(watermark_score, 1.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error in watermark detection: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _check_repetition(self, text: str) -> float:\n",
        "        \"\"\"Check for repetitive patterns.\"\"\"\n",
        "        words = text.lower().split()\n",
        "        if len(words) < 10:\n",
        "            return 0.0\n",
        "\n",
        "        # Check for repeated phrases\n",
        "        bigrams = [f\"{words[i]} {words[i+1]}\" for i in range(len(words)-1)]\n",
        "        trigrams = [f\"{words[i]} {words[i+1]} {words[i+2]}\" for i in range(len(words)-2)]\n",
        "\n",
        "        bigram_counts = Counter(bigrams)\n",
        "        trigram_counts = Counter(trigrams)\n",
        "\n",
        "        # High repetition suggests AI\n",
        "        max_bigram_count = max(bigram_counts.values()) if bigram_counts else 0\n",
        "        max_trigram_count = max(trigram_counts.values()) if trigram_counts else 0\n",
        "\n",
        "        repetition_score = 0.0\n",
        "        if max_bigram_count > 3:\n",
        "            repetition_score += 0.3\n",
        "        if max_trigram_count > 2:\n",
        "            repetition_score += 0.5\n",
        "\n",
        "        return min(repetition_score, 1.0)\n",
        "\n",
        "    def _check_perplexity(self, text: str) -> float:\n",
        "        \"\"\"Check perplexity indicators.\"\"\"\n",
        "        # Simple heuristic: very consistent sentence lengths might indicate AI\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) < 3:\n",
        "            return 0.0\n",
        "\n",
        "        sentence_lengths = [len(word_tokenize(sent)) for sent in sentences]\n",
        "        if len(sentence_lengths) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Calculate coefficient of variation\n",
        "        mean_length = np.mean(sentence_lengths)\n",
        "        std_length = np.std(sentence_lengths)\n",
        "\n",
        "        if mean_length == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cv = std_length / mean_length\n",
        "\n",
        "        # Very low variation might indicate AI\n",
        "        if cv < 0.3:\n",
        "            return 0.5\n",
        "        return 0.0\n",
        "\n",
        "    def _check_token_patterns(self, text: str) -> float:\n",
        "        \"\"\"Check for token patterns.\"\"\"\n",
        "        # Check for overly formal language patterns\n",
        "        formal_patterns = [\n",
        "            r'\\b(however|furthermore|moreover|therefore|consequently|nevertheless)\\b',\n",
        "            r'\\b(in conclusion|in summary|to summarize|in other words)\\b',\n",
        "            r'\\b(it is important to note|it should be noted|it is worth mentioning)\\b'\n",
        "        ]\n",
        "\n",
        "        pattern_count = 0\n",
        "        for pattern in formal_patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            pattern_count += len(matches)\n",
        "\n",
        "        # Normalize by text length\n",
        "        words = len(text.split())\n",
        "        if words == 0:\n",
        "            return 0.0\n",
        "\n",
        "        pattern_density = pattern_count / words\n",
        "        return min(pattern_density * 10, 1.0)  # Scale up the score\n",
        "\n",
        "    def _check_sentence_structure(self, text: str) -> float:\n",
        "        \"\"\"Check sentence structure patterns.\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) < 3:\n",
        "            return 0.0\n",
        "\n",
        "        # Check for overly similar sentence starts\n",
        "        sentence_starts = []\n",
        "        for sent in sentences:\n",
        "            words = word_tokenize(sent.lower())\n",
        "            if len(words) >= 2:\n",
        "                sentence_starts.append(f\"{words[0]} {words[1]}\")\n",
        "\n",
        "        if len(sentence_starts) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        start_counts = Counter(sentence_starts)\n",
        "        max_count = max(start_counts.values())\n",
        "\n",
        "        # If many sentences start similarly, might be AI\n",
        "        if max_count > len(sentences) * 0.3:\n",
        "            return 0.6\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "class StylometricAnalyzer:\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            self.stopwords = set(stopwords.words('english'))\n",
        "        except:\n",
        "            logger.warning(\"NLTK stopwords not available, using basic set\")\n",
        "            self.stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}\n",
        "\n",
        "        self.features = {\n",
        "            'avg_word_length': self._calculate_avg_word_length,\n",
        "            'avg_sentence_length': self._calculate_avg_sentence_length,\n",
        "            'type_token_ratio': self._calculate_type_token_ratio,\n",
        "            'stopword_ratio': self._calculate_stopword_ratio,\n",
        "            'punctuation_ratio': self._calculate_punctuation_ratio,\n",
        "            'sentence_complexity': self._calculate_sentence_complexity\n",
        "        }\n",
        "\n",
        "    def analyze(self, text: str) -> Dict[str, float]:\n",
        "        \"\"\"Analyze stylometric features of text.\"\"\"\n",
        "        features = {}\n",
        "        for feature_name, feature_func in self.features.items():\n",
        "            try:\n",
        "                features[feature_name] = feature_func(text)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error calculating {feature_name}: {e}\")\n",
        "                features[feature_name] = 0.0\n",
        "        return features\n",
        "\n",
        "    def _calculate_avg_word_length(self, text: str) -> float:\n",
        "        \"\"\"Calculate average word length.\"\"\"\n",
        "        words = word_tokenize(text.lower())\n",
        "        words = [word for word in words if word.isalpha()]\n",
        "        if not words:\n",
        "            return 0.0\n",
        "        return sum(len(word) for word in words) / len(words)\n",
        "\n",
        "    def _calculate_avg_sentence_length(self, text: str) -> float:\n",
        "        \"\"\"Calculate average sentence length in words.\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        if not sentences:\n",
        "            return 0.0\n",
        "\n",
        "        total_words = 0\n",
        "        for sentence in sentences:\n",
        "            words = word_tokenize(sentence)\n",
        "            total_words += len([word for word in words if word.isalpha()])\n",
        "\n",
        "        return total_words / len(sentences)\n",
        "\n",
        "    def _calculate_type_token_ratio(self, text: str) -> float:\n",
        "        \"\"\"Calculate type-token ratio (lexical diversity).\"\"\"\n",
        "        words = word_tokenize(text.lower())\n",
        "        words = [word for word in words if word.isalpha()]\n",
        "        if not words:\n",
        "            return 0.0\n",
        "\n",
        "        unique_words = len(set(words))\n",
        "        total_words = len(words)\n",
        "        return unique_words / total_words\n",
        "\n",
        "    def _calculate_stopword_ratio(self, text: str) -> float:\n",
        "        \"\"\"Calculate ratio of stopwords.\"\"\"\n",
        "        words = word_tokenize(text.lower())\n",
        "        words = [word for word in words if word.isalpha()]\n",
        "        if not words:\n",
        "            return 0.0\n",
        "\n",
        "        stopword_count = sum(1 for word in words if word in self.stopwords)\n",
        "        return stopword_count / len(words)\n",
        "\n",
        "    def _calculate_punctuation_ratio(self, text: str) -> float:\n",
        "        \"\"\"Calculate ratio of punctuation marks.\"\"\"\n",
        "        if not text:\n",
        "            return 0.0\n",
        "\n",
        "        punct_count = sum(1 for char in text if char in string.punctuation)\n",
        "        return punct_count / len(text)\n",
        "\n",
        "    def _calculate_sentence_complexity(self, text: str) -> float:\n",
        "        \"\"\"Calculate average sentence complexity (clauses per sentence).\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        if not sentences:\n",
        "            return 0.0\n",
        "\n",
        "        total_clauses = 0\n",
        "        clause_indicators = [',', ';', ':', 'and', 'but', 'or', 'because', 'since', 'while', 'although', 'though', 'if', 'when', 'where', 'that', 'which', 'who']\n",
        "\n",
        "        for sentence in sentences:\n",
        "            clause_count = 1  # Base sentence\n",
        "            sentence_lower = sentence.lower()\n",
        "            for indicator in clause_indicators:\n",
        "                clause_count += sentence_lower.count(indicator)\n",
        "            total_clauses += clause_count\n",
        "\n",
        "        return total_clauses / len(sentences)\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.features = {\n",
        "            'lexical_diversity': self._extract_lexical_diversity,\n",
        "            'readability_score': self._extract_readability_score,\n",
        "            'word_frequency': self._extract_word_frequency,\n",
        "            'sentence_variety': self._extract_sentence_variety,\n",
        "            'semantic_coherence': self._extract_semantic_coherence\n",
        "        }\n",
        "\n",
        "    def extract(self, text: str) -> Dict[str, float]:\n",
        "        \"\"\"Extract features from text.\"\"\"\n",
        "        features = {}\n",
        "        for feature_name, feature_func in self.features.items():\n",
        "            try:\n",
        "                features[feature_name] = feature_func(text)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error extracting {feature_name}: {e}\")\n",
        "                features[feature_name] = 0.0\n",
        "        return features\n",
        "\n",
        "    def _extract_lexical_diversity(self, text: str) -> float:\n",
        "        \"\"\"Extract lexical diversity metrics.\"\"\"\n",
        "        words = word_tokenize(text.lower())\n",
        "        words = [word for word in words if word.isalpha()]\n",
        "        if len(words) < 10:\n",
        "            return 0.0\n",
        "\n",
        "        # Calculate TTR (Type-Token Ratio)\n",
        "        unique_words = len(set(words))\n",
        "        total_words = len(words)\n",
        "        ttr = unique_words / total_words\n",
        "\n",
        "        # Calculate MSTTR (Mean Segmental TTR) for longer texts\n",
        "        if total_words > 100:\n",
        "            segment_size = 50\n",
        "            segments = [words[i:i+segment_size] for i in range(0, len(words), segment_size)]\n",
        "            segment_ttrs = []\n",
        "            for segment in segments:\n",
        "                if len(segment) >= 10:\n",
        "                    seg_ttr = len(set(segment)) / len(segment)\n",
        "                    segment_ttrs.append(seg_ttr)\n",
        "            msttr = np.mean(segment_ttrs) if segment_ttrs else ttr\n",
        "            return msttr\n",
        "\n",
        "        return ttr\n",
        "\n",
        "    def _extract_readability_score(self, text: str) -> float:\n",
        "        \"\"\"Extract readability score.\"\"\"\n",
        "        try:\n",
        "            # Try to use textstat if available\n",
        "            score = flesch_reading_ease(text)\n",
        "            # Normalize to 0-1 range (higher = more readable = potentially more AI)\n",
        "            normalized_score = max(0, min(100, score)) / 100\n",
        "            return normalized_score\n",
        "        except:\n",
        "            # Fallback: simple readability metric\n",
        "            sentences = sent_tokenize(text)\n",
        "            words = word_tokenize(text)\n",
        "\n",
        "            if not sentences or not words:\n",
        "                return 0.0\n",
        "\n",
        "            avg_sentence_length = len(words) / len(sentences)\n",
        "            avg_word_length = sum(len(word) for word in words if word.isalpha()) / len([w for w in words if w.isalpha()])\n",
        "\n",
        "            # Simple readability formula (inverse of complexity)\n",
        "            complexity = (avg_sentence_length * 0.1) + (avg_word_length * 0.2)\n",
        "            readability = max(0, min(1, 1 - (complexity / 10)))\n",
        "            return readability\n",
        "\n",
        "    def _extract_word_frequency(self, text: str) -> float:\n",
        "        \"\"\"Extract word frequency patterns.\"\"\"\n",
        "        words = word_tokenize(text.lower())\n",
        "        words = [word for word in words if word.isalpha()]\n",
        "\n",
        "        if len(words) < 10:\n",
        "            return 0.0\n",
        "\n",
        "        word_counts = Counter(words)\n",
        "\n",
        "        # Calculate frequency distribution metrics\n",
        "        frequencies = list(word_counts.values())\n",
        "\n",
        "        # Zipf's law compliance (natural texts follow Zipf distribution)\n",
        "        sorted_freqs = sorted(frequencies, reverse=True)\n",
        "\n",
        "        # Calculate how well it fits Zipf's law\n",
        "        ranks = range(1, len(sorted_freqs) + 1)\n",
        "\n",
        "        # Expected frequency according to Zipf's law\n",
        "        total_words = sum(sorted_freqs)\n",
        "        expected_freqs = [total_words / (rank * sum(1/r for r in ranks)) for rank in ranks]\n",
        "\n",
        "        # Calculate deviation from Zipf's law\n",
        "        if len(expected_freqs) > 0:\n",
        "            mse = np.mean([(actual - expected) ** 2 for actual, expected in zip(sorted_freqs, expected_freqs)])\n",
        "            max_possible_mse = np.var(sorted_freqs)\n",
        "            if max_possible_mse > 0:\n",
        "                zipf_compliance = 1 - (mse / max_possible_mse)\n",
        "                return max(0, min(1, zipf_compliance))\n",
        "\n",
        "        return 0.5  # Default if calculation fails\n",
        "\n",
        "    def _extract_sentence_variety(self, text: str) -> float:\n",
        "        \"\"\"Extract sentence variety metrics.\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) < 3:\n",
        "            return 0.0\n",
        "\n",
        "        # Analyze sentence length variety\n",
        "        sentence_lengths = [len(word_tokenize(sent)) for sent in sentences]\n",
        "        length_variety = np.std(sentence_lengths) / (np.mean(sentence_lengths) + 1e-10)\n",
        "\n",
        "        return length_variety # Higher variety is good, lower might be AI\n",
        "\n",
        "    def _extract_semantic_coherence(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Extract semantic coherence patterns.\n",
        "        This is a placeholder for a more advanced NLP technique.\n",
        "        For a simple heuristic, we can check for common introductory/concluding phrases\n",
        "        that might make the text feel \"too perfect\" or formulaic.\n",
        "        \"\"\"\n",
        "        coherence_score = 0.0\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Overly generic or formulaic phrases\n",
        "        formulaic_phrases = [\n",
        "            \"in conclusion\", \"to summarize\", \"in essence\", \"ultimately\",\n",
        "            \"as previously discussed\", \"this demonstrates that\", \"it is clear that\",\n",
        "            \"it is important to consider\"\n",
        "        ]\n",
        "\n",
        "        for phrase in formulaic_phrases:\n",
        "            if phrase in text_lower:\n",
        "                coherence_score += 0.1 # Add a small penalty for each found\n",
        "\n",
        "        # Could add checks for repeating concepts too closely without new information\n",
        "        # (requires more advanced NLP like topic modeling or embeddings)\n",
        "\n",
        "        # For now, a very simple heuristic: if a text is very short and contains formulaic phrases,\n",
        "        # it might be a strong indicator.\n",
        "        words = word_tokenize(text_lower)\n",
        "        if len(words) < 50 and coherence_score > 0.3:\n",
        "            coherence_score += 0.3 # Stronger indicator for short, formulaic texts\n",
        "\n",
        "        return min(coherence_score, 1.0)\n",
        "\n",
        "\n",
        "class AITextDetector:\n",
        "    def __init__(self):\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "        self.watermark_detector = WatermarkDetector()\n",
        "        self.stylometric_analyzer = StylometricAnalyzer()\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "\n",
        "        self.feature_weights = {\n",
        "            'watermark_detection': 0.3,\n",
        "            'stylometric_analysis': 0.3,\n",
        "            'feature_extraction': 0.4\n",
        "        }\n",
        "        # Thresholds for converting features to AI indicators (heuristics)\n",
        "        self.stylometric_thresholds = {\n",
        "            'avg_word_length': {'min': 4.0, 'max': 6.0, 'ai_deviation_score': 0.2}, # AI often has consistent lengths\n",
        "            'avg_sentence_length': {'min': 15.0, 'max': 25.0, 'ai_deviation_score': 0.2}, # AI often has consistent lengths\n",
        "            'type_token_ratio': {'min': 0.6, 'max': 0.8, 'ai_deviation_score': 0.2}, # AI can be less diverse or too perfect\n",
        "            'stopword_ratio': {'min': 0.4, 'max': 0.55, 'ai_deviation_score': 0.1}, # AI can have consistent stopword usage\n",
        "            'punctuation_ratio': {'min': 0.02, 'max': 0.05, 'ai_deviation_score': 0.1}, # AI can have consistent punctuation\n",
        "            'sentence_complexity': {'min': 1.2, 'max': 1.8, 'ai_deviation_score': 0.1} # AI can have consistent complexity\n",
        "        }\n",
        "        self.extracted_feature_thresholds = {\n",
        "            'lexical_diversity': {'min': 0.6, 'max': 0.8, 'ai_deviation_score': 0.3},\n",
        "            'readability_score': {'min': 0.6, 'max': 0.9, 'ai_deviation_score': 0.3}, # Very high readability might indicate AI\n",
        "            'word_frequency': {'min': 0.8, 'max': 1.0, 'ai_deviation_score': 0.2}, # Deviation from Zipf's law can indicate AI\n",
        "            'sentence_variety': {'min': 0.1, 'max': 0.5, 'ai_deviation_score': 0.2}, # Low variety might indicate AI\n",
        "            'semantic_coherence': {'min': 0.0, 'max': 0.3, 'ai_deviation_score': 0.3} # High score on semantic coherence indicates AI\n",
        "        }\n",
        "\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict:\n",
        "        \"\"\"Analyze text to determine if it's AI-generated.\"\"\"\n",
        "        preprocessed_text = self.preprocessor.preprocess(text)\n",
        "\n",
        "        if not preprocessed_text.strip():\n",
        "            return {\n",
        "                'is_ai_generated': False,\n",
        "                'confidence_score': 0.0,\n",
        "                'ai_indicators': {},\n",
        "                'detailed_scores': {},\n",
        "                'error': \"Input text is empty or too short after preprocessing.\"\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            watermark_score = self.watermark_detector.detect(preprocessed_text)\n",
        "            stylometric_features = self.stylometric_analyzer.analyze(preprocessed_text)\n",
        "            extracted_features = self.feature_extractor.extract(preprocessed_text)\n",
        "\n",
        "            ai_indicators = {}\n",
        "            detailed_scores = {\n",
        "                'watermark_detection': float(watermark_score),\n",
        "                'stylometric_analysis': stylometric_features,\n",
        "                'feature_extraction': extracted_features\n",
        "            }\n",
        "\n",
        "            # Convert stylometric features to AI indicators\n",
        "            stylometric_ai_score = 0.0\n",
        "            for feature, value in stylometric_features.items():\n",
        "                if feature in self.stylometric_thresholds:\n",
        "                    thresholds = self.stylometric_thresholds[feature]\n",
        "                    if not (thresholds['min'] <= value <= thresholds['max']):\n",
        "                        stylometric_ai_score += thresholds['ai_deviation_score']\n",
        "                        ai_indicators[f'stylometric_anomaly_{feature}'] = True\n",
        "                    else:\n",
        "                         ai_indicators[f'stylometric_anomaly_{feature}'] = False\n",
        "            stylometric_ai_score = min(stylometric_ai_score, 1.0)\n",
        "            detailed_scores['stylometric_combined_score'] = float(stylometric_ai_score)\n",
        "\n",
        "\n",
        "            # Convert extracted features to AI indicators\n",
        "            extracted_ai_score = 0.0\n",
        "            # Lower lexical diversity might be AI\n",
        "            if extracted_features['lexical_diversity'] < self.extracted_feature_thresholds['lexical_diversity']['min']:\n",
        "                extracted_ai_score += self.extracted_feature_thresholds['lexical_diversity']['ai_deviation_score']\n",
        "                ai_indicators['low_lexical_diversity'] = True\n",
        "            elif extracted_features['lexical_diversity'] > self.extracted_feature_thresholds['lexical_diversity']['max']:\n",
        "                 # Or unusually high (if AI aims for \"perfect\" diversity)\n",
        "                extracted_ai_score += self.extracted_feature_thresholds['lexical_diversity']['ai_deviation_score'] * 0.5 # Less severe penalty\n",
        "                ai_indicators['unusually_high_lexical_diversity'] = True\n",
        "            else:\n",
        "                ai_indicators['low_lexical_diversity'] = False\n",
        "\n",
        "\n",
        "            # Very high readability might be AI\n",
        "            if extracted_features['readability_score'] > self.extracted_feature_thresholds['readability_score']['max']:\n",
        "                extracted_ai_score += self.extracted_feature_thresholds['readability_score']['ai_deviation_score']\n",
        "                ai_indicators['unusually_high_readability'] = True\n",
        "            else:\n",
        "                ai_indicators['unusually_high_readability'] = False\n",
        "\n",
        "            # Deviation from Zipf's law (word_frequency) might indicate AI\n",
        "            if extracted_features['word_frequency'] < self.extracted_feature_thresholds['word_frequency']['min']:\n",
        "                extracted_ai_score += self.extracted_feature_thresholds['word_frequency']['ai_deviation_score']\n",
        "                ai_indicators['zipf_deviation'] = True\n",
        "            else:\n",
        "                ai_indicators['zipf_deviation'] = False\n",
        "\n",
        "            # Low sentence variety might indicate AI\n",
        "            if extracted_features['sentence_variety'] < self.extracted_feature_thresholds['sentence_variety']['min']:\n",
        "                extracted_ai_score += self.extracted_feature_thresholds['sentence_variety']['ai_deviation_score']\n",
        "                ai_indicators['low_sentence_variety'] = True\n",
        "            else:\n",
        "                ai_indicators['low_sentence_variety'] = False\n",
        "\n",
        "            # High semantic coherence score indicates AI\n",
        "            if extracted_features['semantic_coherence'] > self.extracted_feature_thresholds['semantic_coherence']['max']:\n",
        "                extracted_ai_score += self.extracted_feature_thresholds['semantic_coherence']['ai_deviation_score']\n",
        "                ai_indicators['high_semantic_coherence'] = True\n",
        "            else:\n",
        "                ai_indicators['high_semantic_coherence'] = False\n",
        "\n",
        "\n",
        "            extracted_ai_score = min(extracted_ai_score, 1.0)\n",
        "            detailed_scores['extracted_features_combined_score'] = float(extracted_ai_score)\n",
        "\n",
        "\n",
        "            # Calculate final score\n",
        "            final_score = (\n",
        "                self.feature_weights['watermark_detection'] * watermark_score +\n",
        "                self.feature_weights['stylometric_analysis'] * stylometric_ai_score +\n",
        "                self.feature_weights['feature_extraction'] * extracted_ai_score\n",
        "            )\n",
        "            final_score = min(final_score, 1.0)\n",
        "\n",
        "            # Add AI indicator bonus\n",
        "            ai_indicator_bonus = sum(ai_indicators.values()) * 0.1 # Small bonus for each positive indicator\n",
        "            final_score = min(final_score + ai_indicator_bonus, 1.0)\n",
        "\n",
        "\n",
        "            return {\n",
        "                'is_ai_generated': bool(final_score > 0.55),\n",
        "                'confidence_score': float(final_score),\n",
        "                'ai_indicators': ai_indicators,\n",
        "                'detailed_scores': detailed_scores,\n",
        "                'visualizations': self._generate_text_visualizations(\n",
        "                    watermark_score, stylometric_ai_score, extracted_ai_score, final_score\n",
        "                )\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing text: {str(e)}\")\n",
        "            return {\n",
        "                'is_ai_generated': False,\n",
        "                'confidence_score': 0.0,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def _generate_text_visualizations(self, watermark_score, stylometric_score, extracted_features_score, final_score):\n",
        "        \"\"\"Generate visualizations for text analysis results.\"\"\"\n",
        "        visualizations = {}\n",
        "\n",
        "        try:\n",
        "            # Scores bar chart\n",
        "            scores = {\n",
        "                'Watermark Detection': watermark_score,\n",
        "                'Stylometric Analysis': stylometric_score,\n",
        "                'Feature Extraction': extracted_features_score\n",
        "            }\n",
        "\n",
        "            fig_scores = px.bar(\n",
        "                x=list(scores.keys()),\n",
        "                y=list(scores.values()),\n",
        "                labels={'x': 'Analysis Type', 'y': 'Score'},\n",
        "                title='Text Analysis Scores',\n",
        "                color=list(scores.values()),\n",
        "                color_continuous_scale='RdYlBu_r'\n",
        "            )\n",
        "            fig_scores.update_layout(showlegend=False)\n",
        "            visualizations['analysis_scores'] = fig_scores\n",
        "\n",
        "            # Confidence gauge\n",
        "            fig_gauge = go.Figure(go.Indicator(\n",
        "                mode=\"gauge+number\",\n",
        "                value=final_score,\n",
        "                domain={'x': [0, 1], 'y': [0, 1]},\n",
        "                title={'text': \"AI Generation Confidence\"},\n",
        "                gauge={'axis': {'range': [0, 1]},\n",
        "                       'steps': [\n",
        "                           {'range': [0, 0.3], 'color': \"lightgreen\"},\n",
        "                           {'range': [0.3, 0.7], 'color': \"yellow\"},\n",
        "                           {'range': [0.7, 1], 'color': \"lightcoral\"}],\n",
        "                       'threshold': {'line': {'color': \"red\", 'width': 4}, 'thickness': 0.75, 'value': 0.55}}\n",
        "            ))\n",
        "            visualizations['confidence_gauge'] = fig_gauge\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error generating text visualizations: {e}\")\n",
        "\n",
        "        return visualizations\n",
        "\n",
        "\n",
        "# Initialize detectors globally or within the Gradio function to avoid re-initialization\n",
        "image_detector = ImageDetector()\n",
        "text_detector = AITextDetector()\n",
        "\n",
        "def ai_content_detector_interface(image: Image.Image, text_input: str):\n",
        "    \"\"\"\n",
        "    Main function for the Gradio interface that handles both image and text analysis.\n",
        "    \"\"\"\n",
        "    image_result = None\n",
        "    text_result = None\n",
        "\n",
        "    # Handle image analysis\n",
        "    if image is not None:\n",
        "        # Gradio passes PIL.Image directly, save it temporarily\n",
        "        image_path = \"temp_image.png\"\n",
        "        image.save(image_path)\n",
        "        image_result = image_detector.analyze_image(image_path)\n",
        "        os.remove(image_path) # Clean up temp file\n",
        "    else:\n",
        "        image_result = {\n",
        "            'is_ai_generated': False,\n",
        "            'confidence_score': 0.0,\n",
        "            'error': \"No image provided for analysis.\"\n",
        "        }\n",
        "\n",
        "    # Handle text analysis\n",
        "    if text_input and text_input.strip():\n",
        "        text_result = text_detector.analyze_text(text_input)\n",
        "    else:\n",
        "        text_result = {\n",
        "            'is_ai_generated': False,\n",
        "            'confidence_score': 0.0,\n",
        "            'error': \"No text provided for analysis.\"\n",
        "        }\n",
        "\n",
        "    # Format output for Gradio\n",
        "    image_output_str = f\"**Image Analysis:**\\n\" \\\n",
        "                       f\"Is AI Generated: {image_result['is_ai_generated']}\\n\" \\\n",
        "                       f\"Confidence Score: {image_result['confidence_score']:.2f}\\n\"\n",
        "\n",
        "    if 'error' in image_result:\n",
        "        image_output_str += f\"Error: {image_result['error']}\\n\"\n",
        "    else:\n",
        "        image_output_str += f\"AI Indicators: {json.dumps(image_result['ai_indicators'], indent=2, cls=NumpyEncoder)}\\n\" \\\n",
        "                            f\"Detailed Scores: {json.dumps(image_result['detailed_scores'], indent=2, cls=NumpyEncoder)}\\n\"\n",
        "\n",
        "    text_output_str = f\"**Text Analysis:**\\n\" \\\n",
        "                      f\"Is AI Generated: {text_result['is_ai_generated']}\\n\" \\\n",
        "                      f\"Confidence Score: {text_result['confidence_score']:.2f}\\n\"\n",
        "\n",
        "    if 'error' in text_result:\n",
        "        text_output_str += f\"Error: {text_result['error']}\\n\"\n",
        "    else:\n",
        "        text_output_str += f\"AI Indicators: {json.dumps(text_result['ai_indicators'], indent=2, cls=NumpyEncoder)}\\n\" \\\n",
        "                           f\"Detailed Scores: {json.dumps(text_result['detailed_scores'], indent=2, cls=NumpyEncoder)}\\n\"\n",
        "\n",
        "\n",
        "    # Prepare visualizations for Gradio's Plot component\n",
        "    image_analysis_scores_plot = image_result.get('visualizations', {}).get('analysis_scores')\n",
        "    image_confidence_gauge_plot = image_result.get('visualizations', {}).get('confidence_gauge')\n",
        "\n",
        "    text_analysis_scores_plot = text_result.get('visualizations', {}).get('analysis_scores')\n",
        "    text_confidence_gauge_plot = text_result.get('visualizations', {}).get('confidence_gauge')\n",
        "\n",
        "    return (image_output_str, image_analysis_scores_plot, image_confidence_gauge_plot,\n",
        "            text_output_str, text_analysis_scores_plot, text_confidence_gauge_plot)\n",
        "\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=ai_content_detector_interface,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Upload Image (Optional)\"),\n",
        "        gr.Textbox(lines=10, label=\"Enter Text Here (Optional)\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Image Analysis Result\"),\n",
        "        gr.Plot(label=\"Image Analysis Scores\"),\n",
        "        gr.Plot(label=\"Image Confidence Gauge\"),\n",
        "        gr.Textbox(label=\"Text Analysis Result\"),\n",
        "        gr.Plot(label=\"Text Analysis Scores\"),\n",
        "        gr.Plot(label=\"Text Confidence Gauge\")\n",
        "    ],\n",
        "    title=\"AI Generated Content Detector\",\n",
        "    description=\"Upload an image and/or enter text to detect if it's AI-generated.\",\n",
        "    live=False # Set to True if you want live updates as text is typed\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(share=True) # `share=True` generates a public link"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dOj5uo0IGkAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}